The pipeline for estimating the state over time uses two different optimization techniques as well as an extended Kalman filter. To initialize the estimation process, the Riemannian elevator algorithm is run once to generate estimations of the positions of the robots based only on distance measurements. These estimations are in practice never optimal points of the stress function, so they are refined by running gradient descent with step sizes according to Kruskal's procedure. 

The result of this is used as the prior $\mathbf{x}_0$, where the angles $\theta_0^{(i)}$ are initialized randomly. An extended Kalman filter (EKF) is then initialized with these mean priors, and high angle uncertainty, which completes the initialization step. The Kalman filter and its extensions try to estimate the state of a system with alternating prediction and update steps. They provide an estimate and the uncertainty of that estimate of a state (mean and covariance in literature), given measurements of that state. A familiarity with Bayesian filtering in general, and the EKF in particular, will henceforth be assumed. 

While tracking the robots, a combination of the EKF and Kruskal gradient descent is used. In accordance with~\cite{R_elevator}, this paper uses the weighted stress function in the gradient descent, which trivially changes the gradients defined in section~\ref{sec:related-work} by adding weights to each element in the sum. The predicted robot poses $\hat{\mathbf{x}}_{t+1 \mid t}$ based on the current pose estimations $\hat{\mathbf{x}}_{t \mid t}$ and the control signals $\mathbf{u}_t$ is determined by the EKF prediction step. After the robots have then moved and taken new distance measurements at the next time, the position prediction is used as the initial guess for Kruskal's gradient descent algorithm. The optimal positions $\mathbf{z}_t$ are then fed to the EKF as the measurements, after which the process repeats. 

The functions used in the EKF are shown below, where the index $(i)$ has been dropped for readability. These functions describe the change of the state of the robots from one time step and the next, and the measurement of that state at a given time step.
% To estimate the state $\totstate$ over time we will use a extended Kalman filter (EKF). However, it will not use the distances between the robots as measurement variable. Instead, it will use the estimated positions that the optimizer algorithms determine. All robots operate independently, but we will use a single filter to estimate all of their states. With this said, update equations for a single robot $i$ are defined, below where the index $(i)$ is dropped for readability:
\begin{align}
    \mathbf{x}_{t+1} &= \f(\mathbf{x}_t, \totu) + \W \\
    \totmeas[t+1] &= \g(\totstate) + \V
\end{align}
where
\begin{align}
    g(\totstate) &= \begin{bmatrix}
        1 & 0 & 0 \\
        0 & 1 & 0
    \end{bmatrix} \totstate \\
    \W &\sim \text{GWN} (0, \Q) \\
    \V &\sim \text{GWN} (0, \R) 
\end{align}
and where $\W$ is uncorrelated with $\V$, as well as both being uncorrelated with noise in different robots. By linearizing $f$, we get the Jacobian matrices needed by the EKF:
\begin{align}
    \mathbf{F}_t &=\begin{bmatrix}
        1 & 0 & -\Delta t v_{t} \sin(\hat{\theta}_{t \mid t}) \\
        0 & 1 & \Delta t v_{t} \cos(\hat{\theta}_{t \mid t}) \\
        0 & 0 & 1
    \end{bmatrix} \\
    \mathbf{H}_t &= \begin{bmatrix}
        1 & 0 & 0 \\
        0 & 1 & 0
    \end{bmatrix}
\end{align}
where $\hat{\theta}_{t \mid t}$ is the Kalman estimate of $\theta_t$ given measurements $\totmeas$. Not that here we are assuming that the measurements of the system are not distance measurements, but instead measurements of the robot positions. We get these positions measurements by using two optimization algorithms. 

% As described in section~\ref{sec:related-work}, this is a non-convex. However, the Riemannian Elevator~\cite{R_elevator} provides an approximate solution as well as a non-trivial lower bound on the problem which we can use to determine the quality of the solution. Given this, we can approximately solve the localization problem for the initial positions. The estimates can be further refined using gradient descent on the stress minimization problem as described in previous sections. 

Below in algorithm~\ref{algo:estimation}is a summary of the tracking pipeline. For conciseness, we define the function $\text{RE}(C, \tilde{D}, W)$ the output of the Riemannian elevator, and $\text{KA}(\hat{\mathbf{x}}_{t \mid t-1}, \mathbf{y}_t, \bm{\psi}_t)$ as the output of gradient descent using Kruskal's algorithm. 

\begin{algorithm}[ht]
    \caption{Online estimation}\label{algo:estimation}
    \textbf{Inputs:} Connectivity matrix $\mathbf{J}_0$, measurement vector $\mathbf{y}_0$, noise power vector $\bm{\psi}_0$
    
    \textbf{Initialize:} Get an initial guess
    \begingroup\setstretch{1.2} % Increase line spacing
    \begin{algorithmic}[1]
        \Statex \underline{Find initial guess with Riemannian elevator}
        \State Construct matrices $C$, $\tilde{D}$, and $W$, see \cite{R_elevator}
        \State $\hat{\mathbf{X}}_0 \ \leftarrow\ \text{RE}(C, \tilde{D}, W)$
        \State Recover $\hat{\mathbf{x}}_{0 \mid 0} \in \R{3n}$ from $\hat{\mathbf{X}}_0 \in \R{n \times 2}$, assuming $\hat{\theta}^{(i)} = 0$ with high uncertainty.
    \end{algorithmic}

    \textbf{Track:} Continuously track the states
    \begin{algorithmic}[1]
        \Statex \underline{Predict}
        \State $\hat{\mathbf{x}}_{t \mid t-1} = \mathbf{f}(\hat{\mathbf{x}}_{t-1 \mid t-1}, \mathbf{u}_{t-1})$
        \State $\mathbf{P}_{t \mid t-1} = \mathbf{F}_t \mathbf{P}_{t-1 \mid t-1} \mathbf{F}_t^\top + \mathbf{Q}_t$
        \Statex \underline{Measure}
        \State Collect distance measurements $\mathbf{y}_t$
        \State Get position measurements $\mathbf{z}_t = \text{KA}(\hat{\mathbf{x}}_{t \mid t-1}, \mathbf{y}_t, \bm{\psi}_t)$ and corresponding matrices and vectors $\mathbf{J}_t$, $\mathbf{L}_t$, $\bm{\psi}_t$
        \Statex \underline{Update}
        \State $\mathbf{K}_t = \mathbf{P}_{t \mid t-1} \mathbf{H}^\top_t (\mathbf{H}_t \mathbf{P}_{t \mid t-1} \mathbf{H}_t^\top + \mathbf{R}_t)^{-1}$
        \State $\hat{\mathbf{x}}_{t \mid t} = \hat{\mathbf{x}}_{t\mid t-1} + \mathbf{K}_t (\mathbf{z}_t - \mathbf{g}(\hat{\mathbf{x}}_{t \mid t-1}))$
        \State $\mathbf{P}_{t \mid t} = (\mathbf{I} - \mathbf{K}_t \mathbf{H}_t) \mathbf{P}_{t \mid t-1}$
        \Statex \underline{Repeat}: Go to \underline{Predict}
    \end{algorithmic}
    \endgroup
\end{algorithm}
This section deals with the experimental results of the implemented algorithms. 

\subsection{Kruskal's Method}
An important part of the tracking pipeline is the gradient descent optimization step. To measure the performance of Kruskal's method, the speed and reliability was measured and compared with more established step schedules. In figures~\ref{fig:convergence-comp-small}, \ref{fig:convergence-comp-medium}, and~\ref{fig:convergence-comp-large}, these results are shown. 
\begin{figure}[ht]
    \centering
    \includesvg[width=\linewidth]{kruskal_comp_n4_m7.svg}
    \caption{Convergence speed of gradient descent for a small example. Kruskal's method is better. }
    \label{fig:convergence-comp-small}
\end{figure}
\begin{figure}[ht]
    \centering
    \includesvg[width=\linewidth]{kruskal_comp_n10_m45.svg}
    \caption{Convergence speed of gradient descent for a medium example. Performance is even. }
    \label{fig:convergence-comp-medium}
\end{figure}
\begin{figure}[ht]
    \centering
    \includesvg[width=\linewidth]{kruskal_comp_n15_m150.svg}
    \caption{Convergence speed of gradient descent for a large problem. Note that both $\frac{1}{\sqrt{k}}$ step and constant step size are missing the largest value because the objective value diverged.}
    \label{fig:convergence-comp-large}
\end{figure}
It is clear that Kruskal's method carries a performance advantage for smaller problems but it is outperformed by the other two methods for larger problems. However, it should be noted that, while Kruskal's method is superficially complex, it does not require much tuning to work well. The only parameter to choose is the initial step, whose value is not dependent on problem size. Meanwhile, for the other two algorithms to be faster, they require tuning for each specific problem instance. In the largest case shown in the plots, neither constant step size nor $\frac{1}{\sqrt{k}}$ step size converged for one the the chosen parameter values. This illustrates the perhaps most important property of Kruskal's algorithm: it works well for all problem sizes without tuning. 

\subsection{Initial Guess}
The Riemannian elevator was chosen to provide certifiable, accurate estimations of the initial positions, since the tracking performance hinges on the accuracy of this initial guess. In figure~\ref{fig:accuracy-comp}, the initial guess performance is shown. The plot was generated by running the Riemannian elevator followed by a refinedment 
\begin{figure}
    \centering
    \includesvg[width=\linewidth]{accuracy_comparison.svg}
    \caption{Accuracy of the Riemannian elevator compared to a random (informed) guess.}
    \label{fig:accuracy-comp}
\end{figure}
As is clearly shown in figure~\ref{fig:accuracy-comp}, the Riemannian elevator outperforms randomly guessing an \textbf{CONTINUE HERE AAAAAAAAAA AAAAAAA AAAAAAAAAA AAAAAAAA AAAAAAAA AAAAAAAAAAA AAAAAA AAAAAAAAA AAAAAAAA AAAAAAAAA AAAAAAAAAA AAAAAAAAAA AAAAAA AAAAAA AAAAAAAAA AAA AAAAAAAA AAAA AAAAAAA AAAAAAA AAAA AAAA AA AAAAAAA A AAAAA AAAAAAA}